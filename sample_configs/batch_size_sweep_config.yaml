- config:
    vllm_params:
      modelid: "meta-llama/Meta-Llama-3.1-8B-Instruct"
      max_sequence_length: 2048
      tensor_parallel_degree: 8
      batch_size: "sweep"
    benchmark_params:
      num_input_tokens: 1024
      num_output_tokens: 512
      concurrency: "sweep"
      results_path: "test"
    name: "bs_sweep"